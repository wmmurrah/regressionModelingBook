[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Design and Analysis: A Computational Model-Based Approach",
    "section": "",
    "text": "Preface\nThis book was created to:\nA traditional teaching method involves having student conduct simple statistical algorithms by hand, meaning working out the calculations step-by-step on paper, as to develop an intuition of the methods, and what they are doing. I take a computational approach in an attempt to achieve the same outcome, by encouraging readers to program the algorithms using the computer."
  },
  {
    "objectID": "index.html#model-based-approach",
    "href": "index.html#model-based-approach",
    "title": "Design and Analysis: A Computational Model-Based Approach",
    "section": "Model-Based Approach",
    "text": "Model-Based Approach\nA major goal of this book is that, after working through it, the researcher has a firm grasp of the general linear model, as a foundation to scientific modeling."
  },
  {
    "objectID": "index.html#computational-approach",
    "href": "index.html#computational-approach",
    "title": "Design and Analysis: A Computational Model-Based Approach",
    "section": "Computational Approach",
    "text": "Computational Approach\nI use a programming language, not only to do statistics but as a tool to learn statistics.\nsimulation is used to develop intuitions about statistical methods."
  },
  {
    "objectID": "index.html#project-based-approach",
    "href": "index.html#project-based-approach",
    "title": "Design and Analysis: A Computational Model-Based Approach",
    "section": "Project-Based Approach",
    "text": "Project-Based Approach\nProject-based examples,"
  },
  {
    "objectID": "index.html#other-approaches-utilized-in-this-book",
    "href": "index.html#other-approaches-utilized-in-this-book",
    "title": "Design and Analysis: A Computational Model-Based Approach",
    "section": "Other Approaches Utilized in this Book",
    "text": "Other Approaches Utilized in this Book\nTo the extent possible, I have tried to use empirical studies and other published work when developing projects and research examples. I prioritized articles that provide access to raw data and computer code, as this not only helps readers see how information moves from raw data to peer reviewed publications, but also gives readers “hands-on” experience with the data interpretation process.\nI have also utilized research studies of varied quality and challenge readers to evaluate the quality of the studies and give their justification.\nI have also scaffolded the various learning objective of the book. For example, I make the transition from the simulated/coceptual example, to the demonstrated empirical example to the empirical project fairly straight-forward early in the book, but less apparent as readers gain skill in this process."
  },
  {
    "objectID": "intro.html#a-little-philosophy-of-science",
    "href": "intro.html#a-little-philosophy-of-science",
    "title": "1  Introduction to Scientific Modeling",
    "section": "1.1 A little philosophy of science",
    "text": "1.1 A little philosophy of science\nTo me, reality consists of all that exists. This includes, physical objects and processes as well as things like ideas, feeling, and beliefs. So, to me, reality is unitary by definition – there is one reality. But it is extremely complicated. I suspect that it is so complicated that we may never be able to fully comprehend it all, as a species, much less any one of us.\nDoes this mean we can understand reality at all? I don’t think so. I don’t fully understand how my car works, but I do have basic ideas that allow me to problem solve issues such as when it won’t start. When we can’t fully understand something, we are left to build an model of that process. I will talk more about models below, but for now think of a model as an oversimplified representation of a much more complex system."
  },
  {
    "objectID": "intro.html#modeling-in-science",
    "href": "intro.html#modeling-in-science",
    "title": "1  Introduction to Scientific Modeling",
    "section": "1.2 Modeling in Science",
    "text": "1.2 Modeling in Science\n\n1.2.1 Modeling Workflow (McElreath, 2023)\n\nDefine a generative model of the sample,\nDefine specific estimand,\nDefine statistical model to produce estimand,\nTest statistical model (3) using generative model (1),\nAnalyze and summarize sample."
  },
  {
    "objectID": "intro.html#research-design-and-analysis",
    "href": "intro.html#research-design-and-analysis",
    "title": "1  Introduction to Scientific Modeling",
    "section": "1.3 Research Design and Analysis",
    "text": "1.3 Research Design and Analysis\n\n1.3.1 Exploratory to Confirmatory Data Analysis Continuum\nsee(Fife and Rodgers 2022)\nA p-value cannot be used for exploratory analyses.\nIf your theoretical framework does not suggest a causal graph, you are very likely not ready for a confirmatory data analysis."
  },
  {
    "objectID": "intro.html#ethics-in-science",
    "href": "intro.html#ethics-in-science",
    "title": "1  Introduction to Scientific Modeling",
    "section": "1.4 Ethics in Science",
    "text": "1.4 Ethics in Science\n\n1.4.1 Pat and Sam\n\n\n\n\nFife, Dustin A., and Joseph Lee Rodgers. 2022. “Understanding the Exploratory/Confirmatory Data Analysis Continuum: Moving Beyond the “Replication Crisis”.” American Psychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886."
  },
  {
    "objectID": "simpleModels.html#variances-covariances-and-correlations",
    "href": "simpleModels.html#variances-covariances-and-correlations",
    "title": "3  Simple Regression Models",
    "section": "3.1 Variances, Covariances, and Correlations",
    "text": "3.1 Variances, Covariances, and Correlations\n\nlibrary(car)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(gridExtra)\ndata(\"Davis\")\n\n\n3.1.1 Variance\nfor ( \\(n\\) = 1,000, \\(\\bar{X}\\) = 100)\n\n\n\n\n\nThe sample variance is defined as:\n\\[\ns_x^2 = \\frac{\\sum{(X - \\overline{X}})^2}{N - 1} = \\frac{\\sum{x^2}}{N - 1} \\tag{2.1}\n\\]\nwhere \\(X\\) is the variable in question, \\(\\sum{\\mathit{x}^2}\\) is the sum of the squared deviations from the mean of \\(X\\), the latter symbolized as \\(\\overline{X}\\), and \\(N\\) the sample size.\nDefining \\(X\\) and computing the variance “by hand”:\n\nX <- c(65, 69, 67.5, 75, 62.5, 68, 72, 67, 70, 72, 66.5, 68.5) # height\nN <- length(X)\nXbar <- mean(X)\nvar_x <- sum((X - Xbar)^2)/(N - 1)\nvar_x\n\n[1] 11.35606\n\n\nor simply:\n\nvar(X)\n\n[1] 11.35606\n\n\n\n\n3.1.2 Bivariate Relationship\n\n\n\n\n\n\n\n\n\n\n3.1.3 Covariance\nThe covariance of two variables is defined as\n\\[\n  s_{xy} = \\frac{\\sum{(X - \\overline{X})(Y - \\overline{Y})}}{N - 1} = \\frac{\\sum{xy}}{N - 1}. \\tag{2.3}\n\\]\nNote that we can rewrite the variance equation as\n\\[\n  s_{xx} = \\frac{\\sum{(X - \\overline{X})(X - \\overline{X})}}{N - 1} = \\frac{\\sum{xx}}{N - 1},\n\\]\nsuggesting that the variance of a variable can be considered its covariance with itself.\n\ndat <- studht[ ,1:2]\nnames(dat) <- c(\"X\", \"Y\")\n\ndat <- within(dat, {\n  `$d_x$` = X -  mean(X)\n  `$d_x^2$` = (X - mean(X))^2\n  `$d_y$` = Y - mean(Y)\n  `$d_y^2$` = (Y - mean(Y))^2\n})\n\ndat <- dat[ ,c(\"X\", \"$d_x$\", \"$d_x^2$\", \"Y\", \"$d_y$\", \"$d_y^2$\")]\nkable(dat, digits = 2)\n\n\n\n\nX\n\\(d_x\\)\n\\(d_x^2\\)\nY\n\\(d_y\\)\n\\(d_y^2\\)\n\n\n\n\n64\n-3.83\n14.69\n65.0\n-3.58\n12.84\n\n\n68\n0.17\n0.03\n69.0\n0.42\n0.17\n\n\n66\n-1.83\n3.36\n67.5\n-1.08\n1.17\n\n\n75\n7.17\n51.36\n75.0\n6.42\n41.17\n\n\n61\n-6.83\n46.69\n62.5\n-6.08\n37.01\n\n\n68\n0.17\n0.03\n68.0\n-0.58\n0.34\n\n\n72\n4.17\n17.36\n72.0\n3.42\n11.67\n\n\n66\n-1.83\n3.36\n67.0\n-1.58\n2.51\n\n\n69\n1.17\n1.36\n70.0\n1.42\n2.01\n\n\n70\n2.17\n4.69\n72.0\n3.42\n11.67\n\n\n66\n-1.83\n3.36\n66.5\n-2.08\n4.34\n\n\n69\n1.17\n1.36\n68.5\n-0.08\n0.01\n\n\n\n\n\n\\(\\sum{X} = 814\\), \\(\\bar{X} = 67.83\\),\n\\(\\sum{Y} = 823\\), \\(\\bar{Y} = 68.58\\),\n\\(\\sum{d_x^2} = 147.67\\), \\(\\sum{d_y^2} = 124.92\\)\n\\[\n  s_x^2 = \\frac{\\sum{x^2}}{N - 1} = \\frac{147.67}{10 - 1} = 13.42  \n\\]\n\\[\n  s_x = \\sqrt{s_x^2} = 3.66\n\\]\n\\[\n  s_y^2 = \\frac{\\sum{y^2}}{N - 1} = \\frac{124.92}{10 - 1} = 11.36  \n\\]\n\\[\n  s_y = \\sqrt{s_y^2} = 3.37\n\\]\n\n\n\n\nX\n\\(d_x\\)\n\\(d_x^2\\)\nY\n\\(d_y\\)\n\\(d_y^2\\)\n\\(d_xd_y\\)\n\n\n\n\n64\n-3.83\n14.69\n65.0\n-3.58\n12.84\n13.74\n\n\n68\n0.17\n0.03\n69.0\n0.42\n0.17\n0.07\n\n\n66\n-1.83\n3.36\n67.5\n-1.08\n1.17\n1.99\n\n\n75\n7.17\n51.36\n75.0\n6.42\n41.17\n45.99\n\n\n61\n-6.83\n46.69\n62.5\n-6.08\n37.01\n41.57\n\n\n68\n0.17\n0.03\n68.0\n-0.58\n0.34\n-0.10\n\n\n72\n4.17\n17.36\n72.0\n3.42\n11.67\n14.24\n\n\n66\n-1.83\n3.36\n67.0\n-1.58\n2.51\n2.90\n\n\n69\n1.17\n1.36\n70.0\n1.42\n2.01\n1.65\n\n\n70\n2.17\n4.69\n72.0\n3.42\n11.67\n7.40\n\n\n66\n-1.83\n3.36\n66.5\n-2.08\n4.34\n3.82\n\n\n69\n1.17\n1.36\n68.5\n-0.08\n0.01\n-0.10\n\n\n\n\n\\(\\sum{X} = 814\\), \\(\\bar{X} = 67.83\\),\n\\(\\sum{Y} = 823\\), \\(\\bar{Y} = 68.58\\),\n\\(\\sum{d_xd_y} = 133.17\\)\n\\[\n  s_{xy} = \\frac{\\sum{(X - \\overline{X})(Y - \\overline{Y})}}{N - 1} =\n\\]\n\\[\n  \\frac{\\sum{xy}}{N - 1} = \\frac{133.17}{9} =12.11.\n\\]\n\n\n3.1.4 Correlation\n\\[r_{XY} = \\frac{\\text{Cov}(XY)}{s_{X}s_{Y}} =\n  \\frac{12.11}{3.66 \\times 3.37} =\n  \\frac{12.11}{12.35} = 0.98.\\]"
  },
  {
    "objectID": "simpleModels.html#conceptual-demonstration-of-simple-regression",
    "href": "simpleModels.html#conceptual-demonstration-of-simple-regression",
    "title": "3  Simple Regression Models",
    "section": "3.2 Conceptual Demonstration of Simple Regression",
    "text": "3.2 Conceptual Demonstration of Simple Regression\nLet’s say we were about to conduct a large study and needed to know the participants heights. But it is very expensive to directly measure each participants height. We could just ask participants to report their height. So, we want to know how accurate people are at reporting their height. To evaluate that we could ask a random sample of people from the study population to report their height and then obtain direct measures of their height and see how strongly those are related. We want to see how well reported height predicts measured height.\nBelow are statistical models of the relationship between measured height (\\(Y\\)) and reported height (\\(X\\)).\nPopulation Model:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\]\n\\[ \\hat{Y_i} = \\beta_0 + \\beta_1 X_i\\]\nSample Model:\n\\[Y_i = b_0 + b_1 X_i + e_i\\] \\[\\hat{Y_i} = b_0 + b_1 X_i\\] Below is a data set of graduate students in a research methods class, were I asked them to report their height (\\(X\\)) and then we measured their height (\\(Y\\)). This data set is much smaller than we would need to adequately answer our research question, but we will use it to demonstrate some key principles of simple regression.\n\ndf <- dat[ ,c(\"X\", \"Y\")]\nkable(df)\n\n\n\n\nX\nY\n\n\n\n\n64\n65.0\n\n\n68\n69.0\n\n\n66\n67.5\n\n\n75\n75.0\n\n\n61\n62.5\n\n\n68\n68.0\n\n\n72\n72.0\n\n\n66\n67.0\n\n\n69\n70.0\n\n\n70\n72.0\n\n\n66\n66.5\n\n\n69\n68.5\n\n\n\n\n\n\\[\n\\text{Cor}_{XY} = 0.98\n\\]\n\ncor(df$X, df$Y)\n\n[1] 0.9804921\n\n\n\n3.2.0.1 Data Summary\n\nkable(psych::describe(df, fast = TRUE), digits = 2)\n\n\n\n\n\nvars\nn\nmean\nsd\nmin\nmax\nrange\nse\n\n\n\n\nX\n1\n12\n67.83\n3.66\n61.0\n75\n14.0\n1.06\n\n\nY\n2\n12\n68.58\n3.37\n62.5\n75\n12.5\n0.97\n\n\n\n\n\n\n\n3.2.1 Graphical Demonstration of Simple Regression\n\nplot(Y ~ X, data = df)\n\n\n\n\n\n\n\n\n\n\nYbar <- mean(df$Y)\n\nYbar\n\n[1] 68.58333\n\n# Empty Model\nemptyModel <- lm(Y ~ 1, data = df)\n\ncoef(emptyModel)\n\n(Intercept) \n   68.58333 \n\n# Predicted value\npredict(emptyModel)\n\n       1        2        3        4        5        6        7        8 \n68.58333 68.58333 68.58333 68.58333 68.58333 68.58333 68.58333 68.58333 \n       9       10       11       12 \n68.58333 68.58333 68.58333 68.58333 \n\n\n\n\n\n\n\n\ndf$d <- df$Y - mean(df$Y)\n\n\n\n\n\n\nX\nY\nd\n\n\n\n\n64\n65.0\n-3.6\n\n\n68\n69.0\n0.4\n\n\n66\n67.5\n-1.1\n\n\n75\n75.0\n6.4\n\n\n61\n62.5\n-6.1\n\n\n68\n68.0\n-0.6\n\n\n72\n72.0\n3.4\n\n\n66\n67.0\n-1.6\n\n\n69\n70.0\n1.4\n\n\n70\n72.0\n3.4\n\n\n66\n66.5\n-2.1\n\n\n69\n68.5\n-0.1\n\n\n\n\n\n\n\n\n\n\n\nsimpleReg <- lm(Y ~ X, data = df)\nprint(coef(simpleReg), digits = 2)\n\n(Intercept)           X \n        7.4         0.9 \n\ndf$resid <- resid(simpleReg)\n\n\n\n\n\n\nX\nY\nd\nresid\n\n\n\n\n64\n65.0\n-3.6\n-0.1\n\n\n68\n69.0\n0.4\n0.3\n\n\n66\n67.5\n-1.1\n0.6\n\n\n75\n75.0\n6.4\n0.0\n\n\n61\n62.5\n-6.1\n0.1\n\n\n68\n68.0\n-0.6\n-0.7\n\n\n72\n72.0\n3.4\n-0.3\n\n\n66\n67.0\n-1.6\n0.1\n\n\n69\n70.0\n1.4\n0.4\n\n\n70\n72.0\n3.4\n1.5\n\n\n66\n66.5\n-2.1\n-0.4\n\n\n69\n68.5\n-0.1\n-1.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Interpreting Output from Simple Regression\nWe need to be able to interpret the coefficients, and determine their implications for our research question. We can look at a summary of the results of the simple regression in R as follows:\n\nsummary(mod)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.13544 -0.36315  0.01185  0.29091  1.46275 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.41084    3.88315   1.908   0.0854 .  \nX            0.90181    0.05717  15.774 2.15e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6947 on 10 degrees of freedom\nMultiple R-squared:  0.9614,    Adjusted R-squared:  0.9575 \nF-statistic: 248.8 on 1 and 10 DF,  p-value: 2.153e-08\n\n\nThere is a lot of information contained in this output. First, note that the multiple R-squared is .96, which is an estimate of the proportion of measured height (\\(Y\\)) that can be explained by reported height (\\(X\\)). We can convert this proportion into a percentage by multiplying it by 100, and state that reported height explains about 96% of the variance in measured height.\nNext, we can look at the intercept under the Coefficients: heading of the output. This value is 7.4 (rounding to 1 decimal place). What does this tell us? The intercept is technically the predicted value of the outcome (\\(Y\\)) when the predictor is zero. I would interpret this to mean that, according to this model, someone who reported their height as zero, would be predicted to have a measured height of a little less than 7 and 1/2 inches. This value is not very meaningful because no graduate students reported a zero height. The intercept is outside the range of the observed data (and theoretical population). Later we will learn how to make the intercept more meaningful. Often, though, the intercept is not the important coefficient. The slope is often what we want to learn about. For this simple regression, the estimated slope is about 0.9. Technically, the slope is the expected change in the outcome for a one unit change in the predictor. For this example, that suggests that, according to this simple regression model, two graduate students who reported their heights as being different by one inch, would be predicted to differ in actual height by about 0.9 inches.\nWe set this up as ## Real Data Example\n\n#install.packages(\"NHANES)\nlibrary(NHANES)\nlibrary(effectsize)\ndata(NHANES)\n\nNext we will look at the variables available in the NHANES dataframe, which is a available in the NHANES package in R. To illustrate how we can make statistical inferences, we can assume we want to know if adult height differs between women and men. Admittedly, this is a ver trivial goal of research, but it will help us to review the topic of statistical inference. To do this we will consider three scenarios.\nFirst, we will pretend that we have no idea about what to expect between variables in this data and are engaging in an exploratory study which includes looking for relations between height and gender.\nSecond, we will pretend that we suspect that there is a relation between height and gender, possibly among other relations, and we want to know if heights are different between the populations of adult women and men. This is a rough confirmatory study.\nThird, we will pretend that a primary goal of our study is to test the hypothesis that men are taller than women. We have a compelling theoretical model that suggest this relation, and we decide to test this hypothesis before we collect or at least before we look at our data. This is a strict confirmatory study.\n\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\nThe names are helpful, but we will want to know more about each variable. Because this is a data set from an R package, you can learn more about it using R’s help system as follows:\n\n?NHANES\n\nYou should run the above command in RStudio and take a few minutes to read through the resulting help file, which will give you an idea of what is contained in the data, an what each variables captures.\nOne thing you might have noticed reading through the data description is that height was measured in centimeters (cm). Below I create a variable I call Height_in that converts from centimeters to inches.\n\nNHANES$Height_in <- NHANES$Height/2.54\n\nLet’s look at the distribution of heights using this real data.\n\nhist(NHANES$Height_in, breaks = \"fd\")\n\n\n\n\nThe data do not look normally distributed. instead, there is a long tail on the left side of the distribution. What do you think is going on here? You may want to read back over the data description. To understand this, take a look at the distribution of the age variable.\n\nhist(NHANES$Age)\n\n\n\n\nThe data contain information on people from birth well into adulthood. Maybe the long left tail of the height distibution is a result of this age distribution. To explore this we can plot height against age.\n\nplot(Height_in ~ Age, NHANES)\n\n\n\n\nThis plot seems consistent with our hunch. From birth to the late teens we see a relation between height and age. For participants older than 18 or so, we don’t see a relation between height and age. Because we are interested in the height of adults we subset the data to only include those of the age of 18.\n\nnhanesAdult <- NHANES[NHANES$Age >= 18, ]\n\n\nplot(Height_in ~ Age, nhanesAdult)\n\n\n\n\nWe can now look at the distribution of heights among adults.\n\nhist(nhanesAdult$Height_in)\n\n\n\n\nThis looks more normally distributed, which is what we would expect from a random sample of adults.\nWe might expect there to be differences in the average heights between males and females. To explore this we can use a boxplot.\n\nboxplot(Height_in ~ Gender, nhanesAdult)\n\n\n\n\nWe do see that the median height of males is greater than that of females. The following code gives us the sample means for each gender category.\n\naggregate(Height_in ~ Gender, data = NHANES, FUN = mean)\n\n  Gender Height_in\n1 female  61.65979\n2   male  65.82336\n\n\nTo review what we learned about statistical inference first we will consider our three scenarios.\n\n\n3.2.3 Exploratory Study\nRecall that if we have no idea about what to expect between variables in this data then we are engaging in an exploratory study which includes looking for relations between height and gender.\nThe appropriate comparisons to make are graphics, means, confidence intervals, and effect sizes of the two group’s heights (Fife and Rodgers 2022).\n\nht.gender_model <- lm(Height_in ~ Gender, data =nhanesAdult)\nboxplot(Height_in ~ Gender, data = nhanesAdult)\n\n\n\nsummary(ht.gender_model)\n\n\nCall:\nlm(formula = Height_in ~ Gender, data = nhanesAdult)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8489  -1.9176  -0.0221   1.9070   9.6572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 63.80166    0.04733 1348.00   <2e-16 ***\nGendermale   5.43876    0.06743   80.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.905 on 7422 degrees of freedom\n  (57 observations deleted due to missingness)\nMultiple R-squared:  0.4671,    Adjusted R-squared:  0.467 \nF-statistic:  6506 on 1 and 7422 DF,  p-value: < 2.2e-16\n\nconfint(ht.gender_model)\n\n                2.5 %    97.5 %\n(Intercept) 63.708878 63.894440\nGendermale   5.306584  5.570939\n\ncohens_d(Height_in ~ Gender, data =nhanesAdult)\n\nWarning: Missing values detected. NAs dropped.\n\n\nCohen's d |         95% CI\n--------------------------\n-1.87     | [-1.93, -1.82]\n\n- Estimated using pooled SD.\n\n\nIt is very important to note that a probabilistic interpretation is NOT appropriate for the p values. The p-value of less than .001 for the gender coefficient in the linear model output, should not be used as evidence against the null hypothesis. It would also be very important to report all the tests and comparisons conducted to be transparent about what was done in the study.\nYou should also explore the residuals of the model. We will talk about more sophisticated ways to explore residuals later, for now. Such studies should be followed collecting new data to use one of the other types of studies described below.\n\nhist(resid(ht.gender_model))\n\n\n\n\n\n\n3.2.4 Rough Confirmatory Study\nIf originally we suspected that there is a relation between height and gender, possibly among other relations of interest, and we want to know if heights are different between the populations of adult women and men, this study could be considered a rough confirmatory study.\nIn that situation it would also be appropriate to use graphs, means, and effect sizes, along with confidence intervals. All of the output we obtained above could be used. The difference is that you may emphasize the confidence intervals of the linear model coefficients\n\nconfint(ht.gender_model)\n\n                2.5 %    97.5 %\n(Intercept) 63.708878 63.894440\nGendermale   5.306584  5.570939\n\n\nSuch studies should be followed by strict confirmatory studies if there is support for any of the hypotheses. All analyses and exploratory techniques should be reported transparently.\n\n\n3.2.5 Strict Confirmatory Study\nThis study requires that specific hypotheses related to theory are posited before the data are collected. Then those, and only those hypotheses should be tested and interpreted in the manner that follows. Any additional analyses should be labeled as exploratory and interpreted as in the exploratory study section above. This includes adding any sub-group analyses. If you just happen to find another predictor, say a variable you entered as a covariate to reduce model error, that happens to have a small p-value, it is not appropriate to interpret that as evidence against a null hypothesis, because you did not posit that hypothesis. Such an error is known as HARKing, which stands for Hypothesizing After the Results are Known. It’s bad. Really bad.\n\nsummary(ht.gender_model)\n\n\nCall:\nlm(formula = Height_in ~ Gender, data = nhanesAdult)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8489  -1.9176  -0.0221   1.9070   9.6572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 63.80166    0.04733 1348.00   <2e-16 ***\nGendermale   5.43876    0.06743   80.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.905 on 7422 degrees of freedom\n  (57 observations deleted due to missingness)\nMultiple R-squared:  0.4671,    Adjusted R-squared:  0.467 \nF-statistic:  6506 on 1 and 7422 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nFife, Dustin A., and Joseph Lee Rodgers. 2022. “Understanding the Exploratory/Confirmatory Data Analysis Continuum: Moving Beyond the “Replication Crisis”.” American Psychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fife, Dustin A., and Joseph Lee Rodgers. 2022. “Understanding the\nExploratory/Confirmatory Data Analysis Continuum: Moving Beyond the\n“Replication Crisis”.” American\nPsychologist 77 (3): 453–66. https://doi.org/10.1037/amp0000886."
  }
]