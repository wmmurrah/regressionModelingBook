---
title: "Simple Linear Models"
editor: 
  markdown: 
    wrap: sentence
bibliography: regmodbook.bib    
---

## Variances, Covariances, and Correlations

```{r}
#| warning: false
library(car)
library(dplyr)
library(knitr)
library(ggplot2)
library(gridExtra)
data("Davis")
```

### Variance 

for ( $n$ = 1,000, $\bar{X}$ = 100)

```{r , echo=FALSE}
par(mfrow = c(1, 2))
set.seed(1234)
V <- rnorm(1000, 100, 2)
hist(V, col = "skyblue", breaks = 30, xlim = c(60, 140), ylim = c(0, 100))
abline(v = mean(V), col = "red", lwd = 3)
W <- rnorm(1000, 100, 10)
hist(W, col = "skyblue", breaks = 30, xlim = c(60, 140), 
     ylim = c(0, 100))
abline(v = mean(W), col = "red", lwd = 3)
```

The sample variance is defined as:
  
$$
s_x^2 = \frac{\sum{(X - \overline{X}})^2}{N - 1} = \frac{\sum{x^2}}{N - 1} \tag{2.1}
$$
  
  
where $X$ is the variable in question, $\sum{\mathit{x}^2}$ is the sum of the squared deviations from the mean of $X$, the latter symbolized as $\overline{X}$, and $N$ the sample size. 


Defining $X$ and computing the variance "by hand":

```{r}
X <- c(65, 69, 67.5, 75, 62.5, 68, 72, 67, 70, 72, 66.5, 68.5) # height
N <- length(X)
Xbar <- mean(X)
var_x <- sum((X - Xbar)^2)/(N - 1)
var_x
```

or simply:
```{r}
var(X)
```


### Bivariate Relationship

```{r , echo=FALSE}
# set.seed(123)
# idx <- sample(nrow(Davis), 10)
# idx <- c(179, 14, 195, 170, 40, 118, 43, 194, 153, 90)
# dat <- Davis[idx, c(3,5)]
# rownames(dat) <- NULL
studht <- read.csv("https://raw.githubusercontent.com/wmmurrah/regressionModelingBook/main/data/studentHeight.csv",
                   header = TRUE)

```


```{r, echo = FALSE, out.height="100%"}

studhtxy <- studht[ ,1:3]
names(studhtxy) <- c("repht(X)", "height(Y)", "Gender")
plt <- ggplot(studhtxy, aes(x = `repht(X)`, y = `height(Y)`)) + geom_point() + 
  ylab("Measured Height") + xlab("Reported Height") 

grid.arrange(tableGrob(studhtxy), plt, ncol = 2, widths = c(1,1.5))
```
### Covariance

The covariance of two variables is defined as

$$
  s_{xy} = \frac{\sum{(X - \overline{X})(Y - \overline{Y})}}{N - 1} = \frac{\sum{xy}}{N - 1}. \tag{2.3}
$$
  
Note that we can rewrite the variance equation as 

$$
  s_{xx} = \frac{\sum{(X - \overline{X})(X - \overline{X})}}{N - 1} = \frac{\sum{xx}}{N - 1},
$$
  
suggesting that the variance of a variable can be considered its covariance with itself.

```{r exdat}
dat <- studht[ ,1:2]
names(dat) <- c("X", "Y")

dat <- within(dat, {
  `$d_x$` = X -  mean(X)
  `$d_x^2$` = (X - mean(X))^2
  `$d_y$` = Y - mean(Y)
  `$d_y^2$` = (Y - mean(Y))^2
})

dat <- dat[ ,c("X", "$d_x$", "$d_x^2$", "Y", "$d_y$", "$d_y^2$")]
kable(dat, digits = 2)
```

$\sum{X} = `r sum(dat$X)`$,
$\bar{X} = `r round(mean(dat$X),2)`$,

$\sum{Y} = `r sum(dat$Y)`$,
$\bar{Y} = `r round(mean(dat$Y),2)`$,

$\sum{d_x^2} = `r round(sum(dat[3]),2)`$,
$\sum{d_y^2} = `r round(sum(dat[6]),2)`$ 
  
$$
  s_x^2 = \frac{\sum{x^2}}{N - 1} = \frac{`r round(sum(dat[3]),2)`}{10 - 1} = `r round(var(dat$X),2)`  
$$

$$
  s_x = \sqrt{s_x^2} = `r round(sd(dat$X),2)`
$$

$$
  s_y^2 = \frac{\sum{y^2}}{N - 1} = \frac{`r round(sum(dat[6]),2)`}{10 - 1} = `r round(var(dat$Y),2)`  
$$
  
$$
  s_y = \sqrt{s_y^2} = `r round(sd(dat$Y),2)`
$$


```{r, results = 'asis', echo= FALSE}

dat$`$d_xd_y$` <- with(dat, `$d_x$`*`$d_y$`)

kable(dat, digits = 2)
```

$\sum{X} = `r sum(dat$X)`$, 
$\bar{X} = `r round(mean(dat$X),2)`$,

$\sum{Y} = `r sum(dat$Y)`$, 
$\bar{Y} = `r round(mean(dat$Y),2)`$, 

$\sum{d_xd_y} = `r round(sum(dat[7]),2)`$ 

$$
  s_{xy} = \frac{\sum{(X - \overline{X})(Y - \overline{Y})}}{N - 1} = 
$$
  

$$
  \frac{\sum{xy}}{N - 1} = \frac{`r round(sum(dat[7]),2)`}{9} =`r round(cov(dat$X, dat$Y),2)`. 
$$

### Correlation

$$r_{XY} = \frac{\text{Cov}(XY)}{s_{X}s_{Y}} = 
  \frac{`r round(cov(dat$X, dat$Y),2)`}{`r round(sd(dat$X),2)` \times `r round(sd(dat$Y),2)`} = 
  \frac{`r round(cov(dat$X, dat$Y),2)`}{`r round(sd(dat$X)*sd(dat$Y),2)`} = `r round(cor(dat$X, dat$Y),2)`.$$

## Conceptual Demonstration of Simple Regression

Let's say we were about to conduct a large study and needed to know the participants heights. 
But it is very expensive to directly measure each participants height. 
We could just ask participants to report their height.
So, we want to know how accurate people are at reporting their height.
To evaluate that we could ask a random sample of people from the study population to report their height and then obtain direct measures of their height and see how strongly those are related.
We want to see how well reported height predicts measured height.

Below are statistical models of the relationship between measured height ($Y$) and reported height ($X$).

Population Model:
  
$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$

$$ \hat{Y_i} = \beta_0 + \beta_1 X_i$$

Sample Model:
  
$$Y_i = b_0 + b_1 X_i + e_i$$
$$\hat{Y_i} = b_0 + b_1 X_i$$
Below is a data set of graduate students in a research methods class, were I asked them to report their height ($X$) and then we measured their height ($Y$).
This data set is much smaller than we would need to adequately answer our research question, but we will use it to demonstrate some key principles of simple regression.

```{r}
#| echo: true

df <- dat[ ,c("X", "Y")]
kable(df)
```

$$
\text{Cor}_{XY} = `r with(df, round(cor(X, Y),2))`
$$

```{r}
cor(df$X, df$Y)
```

#### Data Summary

```{r}
kable(psych::describe(df, fast = TRUE), digits = 2)

```


### Graphical Demonstration of Simple Regression
```{r}
plot(Y ~ X, data = df)
```

```{r}
#| echo: false
Y <- df$Y
X <- df$X
mod0 <- lm(Y ~ 1)
int <- round(coef(mod0), 1)[1]
slope <-0
d <- Y - mean(Y)
# Base plot.
plot(Y ~ X, xaxt = "n", yaxt = "n")
# axis(side = 2, at = 0:12)
# axis(side = 1, at = 0:20)
# Regression line.
abline(mod0, col = 'red', lty = 20)

text(x = 64, y = 69, bquote(bar(Y) == .(int)), col = "red")
# points(x = X, y = predict(mod), col = "red")
# segments(x0 = X, x1 = X, y0 = Y, y1 = predict(mod), col = "blue")
# text(x = X + .4, y = Y - .5*d, labels =round(d,1))
# text(x=5, y=11, bquote(Y[i] == .(int) + y[i]))
# text(x=5, y=9, bquote(hat(Y)[i] == .(int)))
```
```{r}
Ybar <- mean(df$Y)

Ybar

# Empty Model
emptyModel <- lm(Y ~ 1, data = df)

coef(emptyModel)

# Predicted value
predict(emptyModel)
```

```{r}
#| echo: false
int <- round(coef(mod0), 1)[1]
slope <-0
d <- Y - mean(Y)
# Base plot.
plot(X, Y,xlim = c(60, 73), ylim = c(60, 73))
# axis(side = 2, at = 0:12)
# axis(side = 1, at = 0:20)
# Regression line.
abline(mod0, col = 'red', lty = 20)
text(x = 64, y = 69, bquote(bar(Y) == .(int)), col = "red")
points(x = X, y = predict(mod0), col = "red")
segments(x0 = X, x1 = X, y0 = Y, y1 = predict(mod0), col = "orange")
text(x = X + .4, y = Y - .5*d, labels =round(d,1))
text(x= 66, y=72, bquote(Y[i] == .(int) + e[i]))
text(x=66, y=71, bquote(hat(Y)[i] == .(int)))
```

```{r}
df$d <- df$Y - mean(df$Y)
```
```{r, echo=FALSE}
kable(df, digits = 1)
```

```{r, echo=FALSE}
mod <- lm(Y ~ X)
int <- round(coef(mod), 1)[1]
slope <- round(coef(mod), 2)[2]
d <- Y - predict(mod)
# Base plot.
plot(X, Y, xlim = c(60, 73), ylim = c(60, 73))
# axis(side = 2, at = 0:12)
# axis(side = 1, at = 0:20)
# Regression line.
abline(mod)
points(x = X, y = predict(mod), col = "red")
segments(x0 = X, x1 = X, y0 = Y, y1 = predict(mod), col = "blue")
text(x = X + .4, y = Y - .5*d, labels =round(d,1))
text(x=66, y=72, bquote(Y[i] == .(int) + .(slope)*X[i] + epsilon[i]))
text(x=66, y=71, bquote(hat(Y)[i] == .(int) + .(slope)*X[i]))
# segments(x0 = 0, x1 = X[3], y0 = 0, y1 = 0, col="green")
 # text(x = 6.2, y = 1.9, paste("Y = ",round(predict(mod)[3], 1)))
text(x = 6.2, y = 1.9, bquote(hat(Y) == .(round(predict(mod)[3], 1))))
# X line
# segments(x0 = X[3], x1 = X[3], y0 = 1, y1 = predict(mod)[3], col = "green")
text(x = 3, y = .2, paste("X = ", X[3]))
# segments(x0 = 0, x1 = X[3], y0 = Y[3], y1 = Y[3], col="green",
         # lty = 2)
# segments(x0 = 0, x1 = X[3], y0=1, y1 = 1, col = "green")
```

```{r}
simpleReg <- lm(Y ~ X, data = df)
print(coef(simpleReg), digits = 2)

df$resid <- resid(simpleReg)
```

```{r, echo=FALSE}

kable(df, digits = 1)

```

```{r, echo=FALSE}
mod <- lm(Y ~ X)
int <- round(coef(mod), 2)[1]
slope <- round(coef(mod), 2)[2]
d <- Y - predict(mod)
# Base plot.
plot(X, Y, xlim = c(60, 73), ylim = c(60, 73))
# axis(side = 2, at = 0:12)
# axis(side = 1, at = 0:20)
# Regression line.
abline(mod)
points(x = X, y = predict(mod), col = "red")
segments(x0 = X, x1 = X, y0 = Y, y1 = predict(mod), col = "blue")
text(x = X + .4, y = Y - .5*d, labels =round(d,1))
text(x=66, y=72, bquote(Y[i] == .(int) + .(slope)*X[i] + epsilon[i]))
text(x=66, y=71, bquote(hat(Y)[i] == .(int) + .(slope)*X[i]))
# segments(x0 = 0, x1 = X[3], y0 = 0, y1 = 0, col="green")
 # text(x = 6.2, y = 1.9, paste("Y = ",round(predict(mod)[3], 1)))
text(x = 63, y = 63, bquote(Delta*hat(Y) == 0.9),
     col = "green")
# X line
segments(x0 = X[5], x1 = X[5]+1, y0 = predict(mod)[5], y1 = predict(mod)[5], col = "green")
# text(x = 3, y = .2, paste("X = ", X[3]))
text(x = X[5]+.5, y = 61, bquote(Delta*X == 1),col = "green")
# segments(x0 = 0, x1 = X[3], y0 = Y[3], y1 = Y[3], col="green",
         # lty = 2)
# Y line
segments(x0 = X[5]+1, x1 = X[5]+1, y0=predict(mod)[5], y1 = predict(mod)[5]+slope, col = "green")
```

```{r, echo=FALSE}
mod <- lm(Y ~ X)
int <- round(coef(mod), 2)[1]
slope <- round(coef(mod), 2)[2]
d <- Y - predict(mod)
# Base plot.
plot(X, Y, xlim = c(60, 73), ylim = c(60, 73))
# axis(side = 2, at = 0:12)
# axis(side = 1, at = 0:20)
# Regression line.
abline(mod)
points(x = X, y = predict(mod), col = "red")
segments(x0 = X, x1 = X, y0 = Y, y1 = predict(mod), col = "blue")
text(x = X + .4, y = Y - .5*d, labels =round(d,1))
text(x=66, y=72, bquote(Y[i] == .(int) + .(slope)*X[i] + epsilon[i]))
text(x=66, y=71, bquote(hat(Y)[i] == .(int) + .(slope)*X[i]))
# segments(x0 = 0, x1 = X[3], y0 = 0, y1 = 0, col="green")
 # text(x = 6.2, y = 1.9, paste("Y = ",round(predict(mod)[3], 1)))



# Add mean segments
int0 <- round(coef(mod0), 2)[1]
int  <- round(coef(mod), 2)[1]
slope <- round(coef(mod), 2)[2]
d <- Y - predict(mod)
Xmean = round(mean(X), 2)
text(x = Xmean+1.2, y = 62, bquote(bar(X) == .(Xmean)), col = "red")
abline(mod0, col = "red", lty = 20)
abline(v = mean(X), col = "red", lty = 20)
text(x = 62, y = 69, bquote(bar(Y) == .(int0)), col = "red")
```

### Interpreting Output from Simple Regression

We need to be able to interpret the coefficients, and determine their implications for our research question.
We can look at a summary of the results of the simple regression in R as follows:
```{r}
summary(mod)
```

There is a lot of information contained in this output.
First, note that the multiple R-squared is .96, which is an estimate of the proportion of measured height ($Y$) that can be explained by reported height ($X$). 
We can convert this proportion into a percentage by multiplying it by 100, and state that reported height explains about 96% of the variance in measured height.

Next, we can look at the intercept under the `Coefficients:` heading of the output.
This value is `r round(coef(mod)[1],1)` (rounding to 1 decimal place).
What does this tell us?
The intercept is technically the predicted value of the outcome ($Y$) when the predictor is zero. 
I would interpret this to mean that, according to this model, someone who reported their height as zero, would be predicted to have a measured height of a little less than 7 and 1/2 inches.
This value is not very meaningful because no graduate students reported a zero height. 
The intercept is outside the range of the observed data (and theoretical population).
Later we will learn how to make the intercept more meaningful.
Often, though, the intercept is not the important coefficient.
The slope is often what we want to learn about.
For this simple regression, the estimated slope is about `r round(coef(mod)[2],2)`.
Technically, the slope is the expected change in the outcome for a one unit change in the predictor.
For this example, that suggests that, according to this simple regression model, two graduate students who reported their heights as being different by one inch, would be predicted to differ in actual height by about `r round(coef(mod)[2],2)` inches.

We set this up as 
## Real Data Example
```{r}
#install.packages("NHANES)
library(NHANES)
library(effectsize)
data(NHANES)
```

Next we will look at the variables available in the `NHANES` dataframe, which is a available in the NHANES package in R.
To illustrate how we can make statistical inferences, we can assume we want to know if adult height differs between women and men.
Admittedly, this is a ver trivial goal of research, but it will help us to review the topic of statistical inference.
To do this we will consider three scenarios.

First, we will pretend that we have no idea about what to expect between variables in this data and are engaging in an **exploratory** study which includes looking for relations between height and gender.

Second, we will pretend that we suspect that there is a relation between height and gender, possibly among other relations, and we want to know if heights are different between the populations of adult women and men.
This is a **rough confirmatory** study.

Third, we will pretend that a primary goal of our study is to test the hypothesis that men are taller than women.
We have a compelling theoretical model that suggest this relation, and we decide to test this hypothesis *before* we collect or at least before we look at our data.
This is a **strict confirmatory** study.

```{r}
names(NHANES)
```

The names are helpful, but we will want to know more about each variable.
Because this is a data set from an R package, you can learn more about it using R's help system as follows:

```{r}
#| eval: false
?NHANES
```

You should run the above command in RStudio and take a few minutes to read through the resulting help file, which will give you an idea of what is contained in the data, an what each variables captures.

One thing you might have noticed reading through the data description is that height was measured in centimeters (cm).
Below I create a variable I call `Height_in` that converts from centimeters to inches.

```{r}
NHANES$Height_in <- NHANES$Height/2.54
```

Let's look at the distribution of heights using this real data.

```{r}
hist(NHANES$Height_in, breaks = "fd")
```

The data do not look normally distributed.
instead, there is a long tail on the left side of the distribution.
What do you think is going on here?
You may want to read back over the data description.
To understand this, take a look at the distribution of the age variable.

```{r}
hist(NHANES$Age)
```

The data contain information on people from birth well into adulthood.
Maybe the long left tail of the height distibution is a result of this age distribution.
To explore this we can plot height against age.

```{r}
plot(Height_in ~ Age, NHANES)
```

This plot seems consistent with our hunch.
From birth to the late teens we see a relation between height and age.
For participants older than 18 or so, we don't see a relation between height and age.
Because we are interested in the height of adults we subset the data to only include those of the age of 18.

```{r}
nhanesAdult <- NHANES[NHANES$Age >= 18, ]
```

```{r}
plot(Height_in ~ Age, nhanesAdult)
```

We can now look at the distribution of heights among adults.

```{r}
hist(nhanesAdult$Height_in)
```

This looks more normally distributed, which is what we would expect from a random sample of adults.

We might expect there to be differences in the average heights between males and females.
To explore this we can use a boxplot.

```{r}
boxplot(Height_in ~ Gender, nhanesAdult)
```

We do see that the median height of males is greater than that of females.
The following code gives us the sample means for each gender category.

```{r}
aggregate(Height_in ~ Gender, data = NHANES, FUN = mean)
```

To review what we learned about statistical inference first we will consider our three scenarios.

### Exploratory Study

Recall that if we have no idea about what to expect between variables in this data then we are engaging in an **exploratory** study which includes looking for relations between height and gender.

The appropriate comparisons to make are graphics, means, confidence intervals, and effect sizes of the two group's heights [@Fife2022Understandingexploratory/confirmatorydata].

```{r}
ht.gender_model <- lm(Height_in ~ Gender, data =nhanesAdult)
boxplot(Height_in ~ Gender, data = nhanesAdult)
summary(ht.gender_model)
confint(ht.gender_model)
cohens_d(Height_in ~ Gender, data =nhanesAdult)
```

It is very important to note that a probabilistic interpretation  is **NOT** appropriate for  the p values.
The p-value of less than .001 for the gender coefficient in the linear model output, should not be used as evidence against the null hypothesis.
It would also be very important to report all the tests and comparisons conducted to be transparent about what was done in the study.

You should also explore the residuals of the model.
We will talk about more sophisticated ways to explore residuals later, for now.
Such studies should be followed collecting new data to use one of the other types of studies described below.

```{r}
hist(resid(ht.gender_model))
```

### Rough Confirmatory Study

If originally we suspected that there is a relation between height and gender, possibly among other relations of interest, and we want to know if heights are different between the populations of adult women and men, this study could be considered a **rough confirmatory** study.

In that situation it would also be appropriate to use graphs, means, and effect sizes, along with confidence intervals.
All of the output we obtained above could be used. 
The difference is that you may emphasize the confidence intervals of the linear model coefficients

```{r}
confint(ht.gender_model)
```
Such studies should be followed by strict confirmatory studies if there is support for any of the hypotheses.
All analyses and exploratory techniques should be reported transparently.


### Strict Confirmatory Study

This study requires that specific hypotheses related to theory are posited before the data are collected. 
Then those, and only those hypotheses should be tested and interpreted in the manner that follows. 
Any additional analyses should be labeled as exploratory and interpreted as in the exploratory study section above.
This includes adding any sub-group analyses. 
If you just happen to find another predictor, say a variable you entered as a covariate to reduce model error, that happens to have a small p-value, it is not appropriate to interpret that as evidence against a null hypothesis, because you did not posit that hypothesis.
Such an error is known as **HARK**ing, which stands for **H**ypothesizing **A**fter the **R**esults are **K**nown.
It's bad. Really bad.
```{r}
summary(ht.gender_model)
```


